{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighborhood Operations\n",
    "## _A pixel is known by the neighborhood it keeps_\n",
    "\n",
    "All our previous discussions focused on interpreting and manipulating images with respect to each pixel that constituted the image. It is a sound way of looking at the image and gives us many capabilities which we saw in all the previous notebooks.\n",
    "\n",
    "However, this pixel oriented way of looking at an image lacks one aspect which is very important for any understanding of the real world. That aspect is **CONTEXT**. Let's see this using an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading an Image\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread('..//assets//images//gurgaon-2.jpg'), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick a specific pixel from an image we have not seen and try to guess what that pixel represents in the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking a specific pixel\n",
    "\n",
    "pixel = image[1000, 2000, :].reshape((1, 1, 3))\n",
    "print('Shape of the pixel {}'.format(pixel.shape))\n",
    "\n",
    "plt.imshow(pixel)\n",
    "plt.title('Pixel at position 1000, 2000')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can We Take A Guess About Which Object Does This Pixel Belong To?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But How Can We Answer This Question Without Seeing The Image?\n",
    "\n",
    "Ok. I'll show you the image from which this pixel has been taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title('We Love You 3000 Robert Downey Junior')\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have seen the image, let's pick a random sample from the image again and try to make the same guess again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (random.randint(0, image.shape[0]), random.randint(0, image.shape[1]))\n",
    "pixel = image[pos[0], pos[1], :].reshape((1, 1, 3))\n",
    "print('Shape of the pixel {}'.format(pixel.shape))\n",
    "\n",
    "plt.imshow(pixel)\n",
    "plt.title('Pixel at position {}'.format(pos))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a pretty tough task to gauge a lot of information about what the pixel represent just by looking at a pixel from an image. Just by looking at 1 pixel, well we can only tell it's RGB values. \n",
    "\n",
    "While it is true that by looking at RGB values of all the pixels we can do quite a bit of image processing, but any kind of intelligent interpretation of the image (ex: what does the image show? weather it is a cat image or a dog image?) demands us to observe part of an image in **context** with other parts.\n",
    "\n",
    "This context is brought in when we analyze pixels along with their **neighborhood**. Let's see how that happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Expand Our Vision. What is a Neighborhood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighborhood of a pixel is the group of pixel connected to or near the pixel. The figures below show examples of 3x3 and 5x5 neighorhoods.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../assets/drawings/3neighborhood.jpg\" width = 300px/>\n",
    "    <figcaption style = \"text-align:center\">A 3x3 Neighborhood</figcaption>\n",
    "</figure>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../assets/drawings/5neighborhood.jpg\" width = 500px/>\n",
    "    <figcaption style = \"text-align:center\">A 5x5 Neighborhood</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = 291\n",
    "lookup = int((neigh - 1)/2)\n",
    "pixel = (random.randint(lookup, image.shape[0] - lookup), random.randint(lookup, image.shape[1] - lookup))\n",
    "\n",
    "print('Shape of the block {}'.format(neigh, neigh))\n",
    "\n",
    "block = image[pixel[0] - lookup:pixel[0] + lookup + 1, pixel[1] - lookup:pixel[1] + lookup + 1, :].reshape(neigh, neigh, 3)\n",
    "\n",
    "plt.imshow(block)\n",
    "plt.title('{}x{} Neighborhood of Pixel {}'.format(neigh, neigh, pixel))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, when we increase our scope of seeing the image, the amount of information we can obtain from it increases. Not only the amount, but the quality of information also changes as we can derive more meaningful information from a larger context just by seeing it.\n",
    "\n",
    "As humans we can easily see and understand the image, it's part and make associations between different parts in our mind. However, how can we make computers do the same? In order to make the computer **see & understand a neighborhood** of an image, we need to **perform some operations on the neighborhood**.\n",
    "\n",
    "Examples of these operations can be our regular mathematical operators. Average, Maximum, Minimum, Differential, etc. \n",
    "\n",
    "##### Let's see how can we do these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBlock(image, neigh):\n",
    "\n",
    "    lookup = int((neigh - 1)/2)\n",
    "    pixel = (random.randint(lookup, image.shape[0] - lookup), random.randint(lookup, image.shape[1] - lookup))\n",
    "    block = image[pixel[0] - lookup:pixel[0] + lookup + 1, pixel[1] - lookup:pixel[1] + lookup + 1, :].reshape(neigh, neigh, 3)\n",
    "    \n",
    "    return block\n",
    "\n",
    "neigh = 591\n",
    "block = getBlock(image, neigh)\n",
    "\n",
    "# Numpy provides us with convinient functions to perform operations on a N-D Numpy Array. let's use them.\n",
    "\n",
    "maxima = np.max(block)\n",
    "mean = np.mean(block)\n",
    "minima = np.min(block)\n",
    "\n",
    "plt.imshow(block)\n",
    "plt.title('{}x{} Neighborhood of Pixel {} with Max = {}, Min = {} and Mean = {}'.format(neigh, neigh, pixel, \n",
    "                                                                                        maxima, minima, mean))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting each statistic for RGB channels\n",
    "\n",
    "maxima = np.max(block.reshape(block.shape[0]*block.shape[1], 3), axis = 0)\n",
    "mean = np.mean(block.reshape(block.shape[0]*block.shape[1], 3), axis = 0)\n",
    "minima = np.min(block.reshape(block.shape[0]*block.shape[1], 3), axis = 0)\n",
    "\n",
    "plt.imshow(block)\n",
    "plt.title('{}x{} Neighborhood of Pixel {} with Max = {}, Min = {} and Mean = {}'.format(neigh, neigh, pixel, \n",
    "                                                                                        maxima, minima, mean))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put this code in a function and try to get a bunch of samples together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNeighborhoodStats(image, neigh):\n",
    "    \n",
    "    block = getBlock(image, neigh)\n",
    "    \n",
    "    maxima = np.max(block.reshape(-1, 3), axis = 0)\n",
    "    mean = np.mean(block.reshape(-1, 3), axis = 0)\n",
    "    minima = np.min(block.reshape(-1, 3), axis = 0)\n",
    "    \n",
    "    return block, [maxima, minima, mean]\n",
    "\n",
    "neigh = 501\n",
    "\n",
    "f, ax = plt.subplots(5, 1, figsize = (5, 25))\n",
    "for i in range(5):\n",
    "    sample = getNeighborhoodStats(image, neigh)\n",
    "    \n",
    "    ax[i].imshow(sample[0])\n",
    "    ax[i].set_title('Max = {}, Min = {} and Mean = {}'.format(sample[1][0], sample[1][1], sample[1][2]))\n",
    "    ax[i].axis('off')\n",
    "\n",
    "f.suptitle('Samples of a {}x{} Neighborhood'.format(neigh, neigh))\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple runs of the above code show that we can get different statistics for different blocks of an image. These statistics give us an idea of some local properties of the image. **Local to the neighborhood** which we are observing. How can we connect these local properties to the complete image?\n",
    "\n",
    "**What if put these statistics together to form another matrix?**. Let's see how can we do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially by taking these statistics from a block of an image we are **converting that block to a vector similar to a pixel**.\n",
    "\n",
    "<br/>\n",
    "\n",
    "<figure>\n",
    "    <img src=\"../assets/drawings/operationonneigh.jpg\" width = 500px/>\n",
    "    <figcaption style = \"text-align:center\">Operation on a Neighborhood</figcaption>\n",
    "</figure>\n",
    "\n",
    "**With many blocks we get many pixels, with many adjacent blocks we get many adjacent pixels and by combining these pixels in a matrix we make a new image. Let's see this in action**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"https://miro.medium.com/max/1400/1*Fw-ehcNBR9byHtho-Rxbtw.gif\" width = 500px/>\n",
    "    <figcaption style = \"text-align:center\">New matrix is created by doing a sliding window operation on the original matrix. Ref: \n",
    "        <a href=\"https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\">Intuitively Understanding Convolutions for Deep Learning</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImageAfterNeighOperation(image, neigh, op = np.mean):\n",
    "    \n",
    "    lookup = int((neigh - 1)/2)\n",
    "    \n",
    "    new_pixels = []\n",
    "    for i in tqdm_notebook(range(lookup, image.shape[0] - lookup), total = image.shape[0] - 2*lookup, desc = 'Sliding Window operation on the Image'):\n",
    "        new_row = []\n",
    "        for j in range(lookup, image.shape[1] - lookup):\n",
    "            block = image[i - lookup: i + lookup +1, j - lookup: j + lookup +1]\n",
    "            new_row.append(op(block.reshape(block.shape[0]*block.shape[1], 3), axis = 0))\n",
    "        new_pixels.append(new_row)\n",
    "    \n",
    "    new_image = np.array(new_pixels)\n",
    "    new_shape = new_image.shape\n",
    "    new_image = new_image.reshape(-1, 3)\n",
    "    new_image = new_image/np.max(new_image, axis = 0)\n",
    "    print('Output Image Size {}'.format(new_shape))\n",
    "    \n",
    "    return new_image.reshape(new_shape)\n",
    "\n",
    "def rowPlot(images, subtitles, title):\n",
    "    \n",
    "    f, ax = plt.subplots(1, len(images), figsize = (5*len(images), 5))\n",
    "    for i, (x, im) in enumerate(zip(ax, images)):\n",
    "        if(len(im.shape) == 3):\n",
    "            x.imshow(im)\n",
    "        else:\n",
    "            x.imshow(im, cmap = 'gray')\n",
    "        x.set_title(subtitles[i])\n",
    "#         x.axis('off')\n",
    "\n",
    "    f.suptitle(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ = cv2.resize(image, (0, 0), fx = 0.1, fy = 0.1)\n",
    "print(image_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = getImageAfterNeighOperation(image_, 51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowPlot([image_, new_image],\n",
    "        ['Original Image', 'Image After Neighborhood Mean Operation'],\n",
    "        'Applying Neighborhood Operations on Image and Stiching Outputs Back as an Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "- The resultant image is smaller in size than the original image\n",
    "- Average operation has resulted in a blurred image.\n",
    "- The resultant image still resembles the original image.\n",
    "\n",
    "Let's look at what do Max and Min operation lead to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_image = getImageAfterNeighOperation(image_, 11, op = np.max)\n",
    "rowPlot([image_, max_image],\n",
    "        ['Original Image', 'Image After Neighborhood Max Operation'],\n",
    "        'Applying Neighborhood Operations on Image and Stiching Outputs Back as an Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_image = getImageAfterNeighOperation(image_, 11, op = np.min)\n",
    "rowPlot([image_, max_image],\n",
    "        ['Original Image', 'Image After Min Neighborhood Operation'],\n",
    "        'Applying Neighborhood Operations on Image and Stiching Outputs Back as an Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be a lot to wrap our heads around it. Let's dive in and see what's happening using a Matplotlib Animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "\n",
    "neigh = 51\n",
    "lookup = int((neigh - 1)/2)\n",
    "new_shape = (image_.shape[0] - neigh + 1, image_.shape[1] - neigh + 1, 3)\n",
    "\n",
    "new_image = np.full(new_shape, fill_value=0.0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "canvas = FigureCanvas(fig)\n",
    "show = True\n",
    "for i in tqdm_notebook(range(lookup, image_.shape[0] - lookup), total = image_.shape[0] - 2*lookup):\n",
    "    new_row = []\n",
    "    for j in range(lookup, image_.shape[1] - lookup):\n",
    "        img = image_.copy()\n",
    "        cv2.rectangle(img, (j - lookup, i - lookup), (j + lookup, i + lookup), [255, 0, 0], 10)\n",
    "        block = image[i - lookup: i + lookup + 1, j - lookup: j + lookup +1]\n",
    "        new_image[i, j, :] = np.mean(block.reshape(block.shape[0]*block.shape[1], 3), axis = 0)/255.0\n",
    "        \n",
    "        ax[0].imshow(img, interpolation = 'bicubic')\n",
    "        ax[1].imshow(new_image, interpolation = 'bicubic')\n",
    "\n",
    "        canvas.draw()\n",
    "        s, (width, height) = canvas.print_to_buffer()\n",
    "        figure = np.fromstring(s, np.uint8).reshape((height, width, 4))\n",
    "        cv2.imshow('fr', figure)\n",
    "\n",
    "        if cv2.waitKey(int(1000/1000)) & 0xFF == ord('q'):\n",
    "            show = False\n",
    "            break\n",
    "    if show == False:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above animation shows how a new image is derived when we repeatedly perform neighborhood operations on adjacent neighborhoods of an image and stitch the resultant pixels together.\n",
    "\n",
    "In the example we saw how mean operation produces a blured image from the original one. And we also saw a blue matrix sliding over the image. This blue matrix is nothing but a 2D array of shape (neigh X neigh) all filled with 1/(neigh X neigh) so that it can create average of the pixels it slides over.\n",
    "\n",
    "We saw that by changing the operation in the function we could change the output image this means that by changing the constituent of this blue matrix we can change the output. This neighborhood matrix is called **Kernel** and we can slide this kernel and provide an operation based on the **weights** of this kernel (matrix cell constituents) to get a resultant image. \n",
    "\n",
    "This operation and this visualization is a simple example and demo of the great **CONVOLUTIONAL OPERATION**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Operation - The Basic Building Block of CNNs\n",
    "\n",
    "**Interesting Read**:\n",
    "\n",
    "- [Image Kernels](https://setosa.io/ev/image-kernels/)\n",
    "- [The Convolution Operation](https://medium.com/@prvnk10/the-convolution-operation-48d72a382f5a)\n",
    "- [Convolutions with OpenCV and Python](https://www.pyimagesearch.com/2016/07/25/convolutions-with-opencv-and-python/)\n",
    "- [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285)\n",
    "\n",
    "I have tried to give an intuitive and organic motivation of convolution operation. However, the articles mentioned above are worth reading. \n",
    "\n",
    "In a 2D convolution operation a small matrix called **kernel** is slided over an **image** and for each slide some operation is performed between the kernel and the image (sort of an element wise matrix multiplication). The resultant image vectors of this operation when stitched together in 2D create a **resultant image** which is **smaller in dimensions** than the original one.\n",
    "\n",
    "The relation between the dimension of the resultant image and the original one is:\n",
    "\n",
    "$W' = \\frac{W - w + 2P}{S} + 1$, $H' = \\frac{H - h + 2P}{S} + 1$\n",
    "\n",
    "Where,\n",
    "\n",
    "$W', H'$: are the width and height of the resultant image\n",
    "\n",
    "$W, H$: are the width and height of the original image\n",
    "\n",
    "$w, h$: are the width and height of the kernel. This is equal to the neighborhood in our previous examples\n",
    "\n",
    "$P$: is padding. Padding is the number of pixels by which the original image is augmented to keep the size of the resultant image same.\n",
    "\n",
    "$S$: is stride. Stride is the number of pixels the kernel moves in each slide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question still remains: What analytical value did we derive out of this Convolution operation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytical value from convolution operation depends heavily on the kernel used to perform the operation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
