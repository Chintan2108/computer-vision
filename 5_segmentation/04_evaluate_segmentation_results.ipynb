{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Result Evaluation\n",
    "\n",
    "So far, we have seen few segmentation algorithms, both contextual and non-contextual. Both giving different results for sample images and different other criteria. \n",
    "\n",
    "The discussion so far raises a question about how do can we evaluate which segmentation output is better than others. Earlier we saw that we motivated Otsu's thresholding method using the same logic. Sum of intra-class variances is one method which gave us a good way to segment purity and it worked as an unsupervised method.\n",
    "\n",
    "However if we have a ground truth map of the segmented result and we want to compare our segmented output with the ground truth. We can leverage other supervised techniques of evaluating weather a segmented image is good or not. The following can be some criteria which such techniques should fullfill:\n",
    "\n",
    "- Easy to code\n",
    "- Quantifiable output\n",
    "- Suitable for vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection Over Union (IOU)\n",
    "\n",
    "- Simply put IOU is the ratio of intersection and union.\n",
    "- Intersection and union are calculated between the segmented output and ground truth. \n",
    "- This metric ranges between 0 and 1 (both inclusive).\n",
    "- 0 means the output is garbage and 1 means a perfect match.\n",
    "- IOU for given segmented image = $\\frac{\\sum_{i=1}^{N} IOU_{i}}{N}$ . Where N is the number of segments.\n",
    "\n",
    "The following image shows a easy to understand visualization of the IOU metric:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png\" width = 300px/>\n",
    "    <figcaption style = \"text-align:center\">IOU Visualization. Ref: \n",
    "        <a href=\"https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\">PyImageSearch IOU</a>\n",
    "    </figcaption>\n",
    "</figure>\n",
    "\n",
    "The following image shows different cases which result in different values of the metric:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"http://ronny.rest/media/tutorials/localization/ZZZ_IMAGES_DIR/iou_scores.png\" width = 500px/>\n",
    "    <figcaption style = \"text-align:center\">IOU Score Comparison. Ref: \n",
    "        <a href=\"http://ronny.rest/tutorials/module/localization_001/iou/\">Ronny Tutorials IOU</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice Coefficient or F1-Score\n",
    "\n",
    "- Dice Coefficient is just a bit different from IOU.\n",
    "- It is the ratio of the intersection and the sum of pixels from both images.\n",
    "- This metric also ranges between 0 and 1 (both inclusive).\n",
    "- 0 means the output is garbage and 1 means a perfect match.\n",
    "\n",
    "The following image shows a easy to understand visualization of the IOU metric:\n",
    "\n",
    "<figure>\n",
    "    <img src=\"https://miro.medium.com/max/858/1*yUd5ckecHjWZf6hGrdlwzA.png\" width = 300px/>\n",
    "    <figcaption style = \"text-align:center\">Dice Coefficient Visualization. Ref: \n",
    "        <a href=\"https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\">Medium Article</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
